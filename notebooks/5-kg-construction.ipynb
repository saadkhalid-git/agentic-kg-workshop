{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np0plMPXRvoq"
   },
   "source": [
    "# Lesson 5 - Knowledge Graph Construction\n",
    "\n",
    "With all the plans in place, it's time to construct the knowledge graph.\n",
    "\n",
    "## Agent\n",
    "\n",
    "- An agent that proposes a schema for the knowledge graph, based on the established user goal.\n",
    "- Input: `approved_user_goal`, `approved_files`, `approved_construction_plan`\n",
    "- Output: `approved_construction_plan`, a dictionary containing the construction plan for the knowledge graph.\n",
    "- Tools: `get_approved_user_goal`, `get_approved_files`, `sample_file`, \n",
    "        `propose_entity_extraction`, `propose_relationship_extraction`, `approve_proposed_construction_plan`\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. The context is initialized with an `approved_user_goal` and `approved_files`\n",
    "2. For each file, determine whether it represents a node or a relationship.\n",
    "3. For each node file, propose an entity extraction (file --> label, properties).\n",
    "4. For each relationship file, propose a relationship extraction (file --> source and target nodes, relationship type and properties).\n",
    "5. Present the construction proposal and ask for approval.\n",
    "6. The user approves the construction proposal.\n",
    "7. The construction proposal is saved in the context state as `approve_proposed_construction_plan`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "The usual import of needed libraries, loading of environment variables, and connection to Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbwxKypOSBkN"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For OpenAI support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.tools import ToolContext\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "# For type hints\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Convenience libraries for working with Neo4j inside of Google ADK\n",
    "from neo4j_for_adk import graphdb, tool_success, tool_error\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MI_qvZJrSJuR"
   },
   "outputs": [],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "MODEL_GPT_5 = \"openai/gpt-5\"\n",
    "\n",
    "llm = LiteLlm(model=MODEL_GPT_5)\n",
    "\n",
    "# Test LLM with a direct call\n",
    "print(llm.llm_client.completion(model=llm.model, messages=[{\"role\": \"user\", \"content\": \"Are you ready?\"}], tools=[]))\n",
    "\n",
    "print(\"\\nOpenAI ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check connection to Neo4j by sending a query\n",
    "\n",
    "neo4j_is_ready = graphdb.send_query(\"RETURN 'Neo4j is Ready!' as message\")\n",
    "\n",
    "print(neo4j_is_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Structured Data Agent\n",
    "\n",
    "The structured data agent is responsible for constructing the \"domain graph\" from structured CSV files,\n",
    "according to the approved construction plan.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you'll define some helper tools to explicitly work with a list of entity types.\n",
    "\n",
    "- `proposed_entities` will be the result of agent analysis, offered to the user for approval\n",
    "- `approved_entities` will be the final list of entity types to extract from unstructured text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the approved constrction plan should look something like this...\n",
    "approved_construction_plan = {\n",
    "    \"Assembly\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"label\": \"Assembly\", \n",
    "        \"unique_column_name\": \"assembly_id\", \n",
    "        \"properties\": [\"assembly_name\", \"quantity\", \"product_id\"]\n",
    "    }, \n",
    "    \"Part\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"label\": \"Part\", \n",
    "        \"unique_column_name\": \"part_id\", \n",
    "        \"properties\": [\"part_name\", \"quantity\", \"assembly_id\"]\n",
    "    }, \n",
    "    \"Product\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"products.csv\", \n",
    "        \"label\": \"Product\", \n",
    "        \"unique_column_name\": \"product_id\", \n",
    "        \"properties\": [\"product_name\", \"price\", \"description\"]\n",
    "    }, \n",
    "    \"Supplier\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"suppliers.csv\", \n",
    "        \"label\": \"Supplier\", \n",
    "        \"unique_column_name\": \"supplier_id\", \n",
    "        \"properties\": [\"name\", \"specialty\", \"city\", \"country\", \"website\", \"contact_email\"]\n",
    "    }, \n",
    "    \"Contains\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"relationship_type\": \"Contains\", \n",
    "        \"from_node_label\": \"Product\", \n",
    "        \"from_node_column\": \"product_id\", \n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\", \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Is_Part_Of\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"relationship_type\": \"Is_Part_Of\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\", \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Supplied_By\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"part_supplier_mapping.csv\", \n",
    "        \"relationship_type\": \"Supplied_By\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Supplier\", \n",
    "        \"to_node_column\": \"supplier_id\", \n",
    "        \"properties\": [\"supplier_name\", \"lead_time_days\", \"unit_cost\", \"minimum_order_quantity\", \"preferred_supplier\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_neo4j_import_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uniqueness_constraint(\n",
    "    label: str,\n",
    "    unique_property_key: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Creates a uniqueness constraint for a node label and property key.\n",
    "    A uniqueness constraint ensures that no two nodes with the same label and property key have the same value.\n",
    "    This improves the performance and integrity of data import and later queries.\n",
    "\n",
    "    Args:\n",
    "        label: The label of the node to create a constraint for.\n",
    "        unique_property_key: The property key that should have a unique value.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with a status key ('success' or 'error').\n",
    "        On error, includes an 'error_message' key.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Use string formatting since Neo4j doesn't support parameterization of labels and property keys when creating a constraint\n",
    "    constraint_name = f\"{label}_{unique_property_key}_constraint\"\n",
    "    query = f\"\"\"CREATE CONSTRAINT `{constraint_name}` IF NOT EXISTS\n",
    "    FOR (n:`{label}`)\n",
    "    REQUIRE n.`{unique_property_key}` IS UNIQUE\"\"\"\n",
    "    results = graphdb.send_query(query)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nodes_from_csv(\n",
    "    source_file: str,\n",
    "    label: str,\n",
    "    unique_column_name: str,\n",
    "    properties: list[str],\n",
    ") -> Dict[str, Any]:\n",
    "    # load nodes from CSV file by merging on the unique_column_name value \n",
    "    query = f\"\"\"LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
    "    CALL (row) {{\n",
    "        MERGE (n:$($label) {{ {unique_column_name} : row[$unique_column_name] }})\n",
    "        FOREACH (k IN $properties | SET n[k] = row[k])\n",
    "    }} IN TRANSACTIONS OF 1000 ROWS\n",
    "    \"\"\"\n",
    "\n",
    "    results = graphdb.send_query(query, {\n",
    "        \"source_file\": source_file,\n",
    "        \"label\": label,\n",
    "        \"unique_column_name\": unique_column_name,\n",
    "        \"properties\": properties\n",
    "    })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_nodes(node_construction: dict) -> dict:\n",
    "    # {\n",
    "    #     \"construction_type\": \"node\", \n",
    "    #     \"source_file\": \"assemblies.csv\", \n",
    "    #     \"label\": \"Assembly\", \n",
    "    #     \"unique_column_name\": \"assembly_id\", \n",
    "    #     \"properties\": [\"assembly_name\", \"quantity\", \"product_id\"]\n",
    "    # }\n",
    "\n",
    "    # create a uniqueness constraint for the unique_column\n",
    "    uniqueness_result = create_uniqueness_constraint(\n",
    "        node_construction[\"label\"],\n",
    "        node_construction[\"unique_column_name\"]\n",
    "    )\n",
    "\n",
    "    if (uniqueness_result[\"status\"] == \"error\"):\n",
    "        return uniqueness_result\n",
    "\n",
    "    # import nodes from csv\n",
    "    load_nodes_result = load_nodes_from_csv(\n",
    "        node_construction[\"source_file\"],\n",
    "        node_construction[\"label\"],\n",
    "        node_construction[\"unique_column_name\"],\n",
    "        node_construction[\"properties\"]\n",
    "    )\n",
    "\n",
    "    return load_nodes_result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_relationships(relationship_construction: dict) -> Dict[str, Any]:\n",
    "    # {\n",
    "    #     \"construction_type\": \"relationship\", \n",
    "    #     \"source_file\": \"parts.csv\", \n",
    "    #     \"relationship_type\": \"Is_Part_Of\", \n",
    "    #     \"from_node_label\": \"Part\", \n",
    "    #     \"from_node_column\": \"part_id\", \n",
    "    #     \"to_node_label\": \"Assembly\", \n",
    "    #     \"to_node_column\": \"assembly_id\", \n",
    "    #     \"properties\": [\"quantity\"]\n",
    "    # }, \n",
    "\n",
    "    # load nodes from CSV file by merging on the unique_column_name value \n",
    "    from_node_column = relationship_construction[\"from_node_column\"]\n",
    "    to_node_column = relationship_construction[\"to_node_column\"]\n",
    "    query = f\"\"\"LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
    "    CALL (row) {{\n",
    "        MATCH (from_node:$($from_node_label) {{ {from_node_column} : row[$from_node_column] }}),\n",
    "              (to_node:$($to_node_label) {{ {to_node_column} : row[$to_node_column] }} )\n",
    "        MERGE (from_node)-[r:$($relationship_type)]->(to_node)\n",
    "        FOREACH (k IN $properties | SET r[k] = row[k])\n",
    "    }} IN TRANSACTIONS OF 1000 ROWS\n",
    "    \"\"\"\n",
    "    \n",
    "    results = graphdb.send_query(query, {\n",
    "        \"source_file\": relationship_construction[\"source_file\"],\n",
    "        \"from_node_label\": relationship_construction[\"from_node_label\"],\n",
    "        \"from_node_column\": relationship_construction[\"from_node_column\"],\n",
    "        \"to_node_label\": relationship_construction[\"to_node_label\"],\n",
    "        \"to_node_column\": relationship_construction[\"to_node_column\"],\n",
    "        \"relationship_type\": relationship_construction[\"relationship_type\"],\n",
    "        \"properties\": relationship_construction[\"properties\"]\n",
    "    })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_domain_graph(construction_plan: dict) -> dict:\n",
    "    # first, import nodes\n",
    "    node_constructions = [value for value in construction_plan.values() if value['construction_type'] == 'node']\n",
    "    for node_construction in node_constructions:\n",
    "        import_nodes(node_construction)\n",
    "\n",
    "    # second, import relationships\n",
    "    relationship_constructions = [value for value in construction_plan.values() if value['construction_type'] == 'relationship']\n",
    "    for relationship_construction in relationship_constructions:\n",
    "        import_relationships(relationship_construction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_domain_graph(approved_construction_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_agent_instruction = \"\"\"\n",
    "    You are a top-tier algorithm designed for analyzing text files and proposing\n",
    "    the kind of named entities that could be extracted which would be relevant \n",
    "    for a user's goal. \n",
    "    \n",
    "    Entities are people, places, things and qualities, but not quantities. \n",
    "    Your goal is to propose a list of the kind of entities, not the actual instances\n",
    "    of entities.\n",
    "\n",
    "    There are two general approaches to identifying kinds of entities:\n",
    "    - well-known entities: these closely correlate with approved node labels\n",
    "    - discovered entities: these may not exist in the graph schema, but appear consistently in the source text\n",
    "\n",
    "    Design rules for well-known entities:\n",
    "    - always use approved node labels as the kind of entity. For example, if there is an approved label \"Person\", and people appear in the text, then propose \"Person\" as the kind of entity.\n",
    "    - prefer reusing existing node labels rather than creating new ones\n",
    "    \n",
    "    Design rules for discovered entities:\n",
    "    - discovered entities are consistently mentioned in the text and are highly relevant to the user's goal\n",
    "    - always look for entities that would provide more depth or breadth to the existing graph\n",
    "    - for example, if the user goal is to represent social communities and the graph has \"Person\" nodes, look through the text to discover entities that are relevant like \"Location\" or \"Event\"\n",
    "    - avoid quantitive entities that may be better represented as a predicate with a property or an additional property on an existing entity.\n",
    "    - for example, do not propose \"Age\" as a kind of entity. That is better represented as an additional property \"age\" on a \"Person\".\n",
    "\n",
    "    Prepare for the task:\n",
    "    - use the 'get_user_goal' tool to get the user goal\n",
    "    - use the 'get_approved_files' tool to get the list of approved files\n",
    "    - use the 'get_approved_labels' tool to get the approved node labels\n",
    "\n",
    "    Think step by step:\n",
    "    1. Sample some of the files using the 'sample_file' tool to understand the content\n",
    "    2. Consider what well-known entities are mentioned in the text\n",
    "    3. Discover entities that are frequently mentioned in the text that support the user's goal\n",
    "    4. Use the 'set_proposed_entities' tool to save the list of well-known and discovered entities\n",
    "    6. Present the recorded kinds of entities along with justification to the user\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the agent is straightforward. Give it a name and description,\n",
    "then use the instructions and tools you just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_AGENT_NAME = \"ner_schema_agent_v1\"\n",
    "ner_schema_agent = Agent(\n",
    "    name=NER_AGENT_NAME,\n",
    "    description=\"Proposes the kind of named entities that could be extracted from text files.\",\n",
    "    model=llm,\n",
    "    instruction=ner_agent_instruction,\n",
    "    tools=ner_agent_tools, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial state is important in this phase, as the agent is designed to act\n",
    "within a particular phase of an overall workflow.\n",
    "\n",
    "The ner agent will need:\n",
    "\n",
    "- the user goal, extended to mention product reviews and what to look for there\n",
    "- a list of markdown files that have been pre-approved\n",
    "- the approved construction plan from the structured data design phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_agent_initial_state = {\n",
    "    \"approved_user_goal\": {\n",
    "        \"kind_of_graph\": \"supply chain analysis\",\n",
    "        \"description\": \"\"\"A multi-level bill of materials for manufactured products, useful for root cause analysis. \n",
    "        Add product reviews to start analysis from reported issues like quality, difficulty, or durability.\"\"\"\n",
    "    },\n",
    "    \"approved_files\": [\n",
    "        \"product_reviews/gothenburg_table_reviews.md\",\n",
    "        \"product_reviews/helsingborg_dresser_reviews.md\",\n",
    "        \"product_reviews/jonkoping_coffee_table_reviews.md\",\n",
    "        \"product_reviews/linkoping_bed_reviews.md\",\n",
    "        \"product_reviews/malmo_desk_reviews.md\",\n",
    "        \"product_reviews/norrkoping_nightstand_reviews.md\",\n",
    "        \"product_reviews/orebro_lamp_reviews.md\",\n",
    "        \"product_reviews/stockholm_chair_reviews.md\",\n",
    "        \"product_reviews/uppsala_sofa_reviews.md\",\n",
    "        \"product_reviews/vasteras_bookshelf_reviews.md\"\n",
    "    ],\n",
    "    \"approved_construction_plan\": {\n",
    "        \"Product\": {\n",
    "            \"construction_type\": \"node\",\n",
    "            \"label\": \"Product\",\n",
    "        },\n",
    "        \"Assembly\": {\n",
    "            \"construction_type\": \"node\",\n",
    "            \"label\": \"Assembly\",\n",
    "        },\n",
    "        \"Component\": {\n",
    "            \"construction_type\": \"node\",\n",
    "            \"label\": \"Component\",\n",
    "        },\n",
    "        \"Supplier\": {\n",
    "            \"construction_type\": \"node\",\n",
    "            \"label\": \"Supplier\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's run the agent. \n",
    "\n",
    "- use the make_agent_caller to create an execution environment\n",
    "- prompt the agent with a single message that should kick-off the analysis\n",
    "- expect the result to be a proposed list of entity types\n",
    "- but *not* a list of approved entity types\n",
    "\n",
    "The entity types here may vary quite a bit. If you're not happy with the proposal,\n",
    "you can run the cell again to get a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import make_agent_caller\n",
    "\n",
    "ner_agent_caller = await make_agent_caller(ner_schema_agent, ner_agent_initial_state)\n",
    "\n",
    "await ner_agent_caller.call(\"Add product reviews.\")\n",
    "\n",
    "session_end = await ner_agent_caller.get_session()\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "print(\"\\nSession state: \", session_end.state)\n",
    "\n",
    "if PROPOSED_ENTITIES in session_end.state:\n",
    "    print(\"\\nProposed entities: \", session_end.state[PROPOSED_ENTITIES])\n",
    "\n",
    "if APPROVED_ENTITIES in session_end.state:\n",
    "    print(\"\\nApproved entities: \", session_end.state[APPROVED_ENTITIES])\n",
    "else:\n",
    "    print(\"\\nAwaiting approval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're happy with the proposal, you can tell the agent that you approve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ner_agent_caller.call(\"Approve the proposed entities.\")\n",
    "\n",
    "session_end = await ner_agent_caller.get_session()\n",
    "\n",
    "print(\"Session state: \", session_end.state)\n",
    "\n",
    "if APPROVED_ENTITIES in session_end.state:\n",
    "    print(\"\\nApproved entities: \", session_end.state[APPROVED_ENTITIES])\n",
    "else:\n",
    "    print(\"\\nAwaiting approval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROPOSED_FACTS = \"proposed_fact\"\n",
    "APPROVED_FACTS = \"approved_facts\"\n",
    "\n",
    "def add_proposed_fact(approved_subject_label:str,\n",
    "                      proposed_predicate_label:str,\n",
    "                      approved_object_label:str,\n",
    "                      tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Add a proposed type of fact that could be extracted from the files.\n",
    "\n",
    "    A proposed fact type is a tuple of (subject, predicate, object) where\n",
    "    the subject and object are approved entity types and the predicate \n",
    "    is a proposed relationship label.\n",
    "\n",
    "    Args:\n",
    "      approved_subject_label: approved label of the subject entity\n",
    "      proposed_predicate_label: label of the predicate\n",
    "      approved_object_label: approved label of the object entity\n",
    "    \"\"\"\n",
    "    # Guard against invalid labels\n",
    "    approved_entities = tool_context.state.get(APPROVED_ENTITIES, [])\n",
    "    \n",
    "    if approved_subject_label not in approved_entities:\n",
    "        return tool_error(f\"Approved subject label {approved_subject_label} not found. Try again.\")\n",
    "    if approved_object_label not in approved_entities:\n",
    "        return tool_error(f\"Approved object label {approved_object_label} not found. Try again.\")\n",
    "    \n",
    "    current_predicates = tool_context.state.get(PROPOSED_FACTS, {})\n",
    "    current_predicates[proposed_predicate_label] = {\n",
    "        \"subject_label\": approved_subject_label,\n",
    "        \"predicate_label\": proposed_predicate_label,\n",
    "        \"object_label\": approved_object_label\n",
    "    }\n",
    "    tool_context.state[PROPOSED_FACTS] = current_predicates\n",
    "    return tool_success(PROPOSED_FACTS, current_predicates)\n",
    "    \n",
    "def get_proposed_facts(tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Get the proposed types of facts that could be extracted from the files.\"\"\"\n",
    "    return tool_context.state.get(PROPOSED_FACTS, {})\n",
    "\n",
    "\n",
    "def approve_proposed_facts(tool_context:ToolContext) -> dict:\n",
    "    \"\"\"Approve the proposed fact types.\"\"\"\n",
    "    tool_context.state[APPROVED_FACTS] = tool_context.state.get(PROPOSED_FACTS)\n",
    "    return tool_success(APPROVED_FACTS, tool_context.state[APPROVED_FACTS])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_agent_instruction = \"\"\"\n",
    "    You are a top-tier algorithm designed for analyzing text files and proposing\n",
    "    the kind of facts that could be extracted from text that would be relevant \n",
    "    for a user's goal. \n",
    "\n",
    "    Do not propose specific individual facts, but instead propose the general kind \n",
    "    of facts that would be relevant for the user's goal. \n",
    "\n",
    "    For example, do not propose \"ABK likes coffee\" but the general type of fact \"Person likes Beverage\".\n",
    "    \n",
    "    Facts are triplets of (subject, predicate, object) where the subject and object are\n",
    "    approved entity types, and the proposed predicate provides information about\n",
    "    how they are related. For example, a fact type could be (Person, likes, Beverage).\n",
    "\n",
    "    Design rules for facts:\n",
    "    - only use approved entity types or node labels as subjects or objects. Do not propose new types of entities\n",
    "    - the proposed predicate should describe the relationship between the approved subject and object\n",
    "    - the predicate should optimize for information that is relevant to the user's goal\n",
    "    - the predicate must appear in the source text. Do not guess.\n",
    "    - use the 'add_proposed_fact' tool to record each proposed fact\n",
    "\n",
    "    Prepare for the task:\n",
    "    - use the 'get_approved_user_goal' tool to get the user goal\n",
    "    - use the 'get_approved_files' tool to get the list of approved files\n",
    "    - use the 'get_approved_entities' tool to get the list of approved entities\n",
    "\n",
    "    Think step by step:\n",
    "    1. Use the 'get_approved_user_goal' tool to get the user goal\n",
    "    2. Sample some of the approved files using the 'sample_file' tool to understand the content\n",
    "    3. Consider how subjects and objects are related in the text\n",
    "    4. Call the 'add_proposed_fact' tool for each fact you propose\n",
    "    5. Present the proposed types of facts to the user, along with an explanation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_agent_tools = [\n",
    "    get_approved_user_goal, get_approved_files, \n",
    "    get_approved_entities,\n",
    "    sample_file,\n",
    "    add_proposed_fact,\n",
    "    get_proposed_facts,\n",
    "    approve_proposed_facts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACT_AGENT_NAME = \"relevant_fact_agent_v1\"\n",
    "relevant_fact_agent = Agent(\n",
    "    name=FACT_AGENT_NAME,\n",
    "    description=\"Proposes the kind of relevant facts that could be extracted from text files.\",\n",
    "    model=llm,\n",
    "    instruction=fact_agent_instruction,\n",
    "    tools=fact_agent_tools, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend the initial state of the fact agent\n",
    "fact_agent_initial_state = ner_agent_initial_state\n",
    "\n",
    "fact_agent_initial_state.update({\n",
    "    APPROVED_ENTITIES: [\n",
    "        'Product', 'Issue', 'User', 'Review'\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_agent_caller = await make_agent_caller(relevant_fact_agent, fact_agent_initial_state)\n",
    "\n",
    "await fact_agent_caller.call(\"Propose facts.\", True)\n",
    "\n",
    "session_end = await fact_agent_caller.get_session()\n",
    "\n",
    "print(\"\\n---\\n\")\n",
    "\n",
    "print(\"\\nSession state: \", session_end.state)\n",
    "\n",
    "if PROPOSED_FACTS in session_end.state:\n",
    "    print(\"\\nProposed facts: \", session_end.state[PROPOSED_FACTS])\n",
    "else:\n",
    "    print(\"\\nProposed facts not found in session state. Try again.\")\n",
    "\n",
    "if APPROVED_FACTS in session_end.state:\n",
    "    print(\"\\nApproved facts: \", session_end.state[APPROVED_FACTS])\n",
    "else:\n",
    "    print(\"\\nApproved facts not found in session state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await fact_agent_caller.call(\"Approve the proposed fact types.\")\n",
    "\n",
    "session_end = await fact_agent_caller.get_session()\n",
    "\n",
    "print(\"Session state: \", session_end.state)\n",
    "\n",
    "if APPROVED_FACTS in session_end.state:\n",
    "    print(\"\\nApproved fact types: \", session_end.state[APPROVED_FACTS])\n",
    "else:\n",
    "    print(\"\\nFailed to approve fact types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
