{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete LangChain GraphRAG Pipeline\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Building a complete knowledge graph using the ADK workflow from notebooks 1-8\n",
    "2. Implementing LangChain GraphRAG for advanced information retrieval\n",
    "3. Multiple retrieval strategies with Q&A chains\n",
    "\n",
    "## Use Case: Supply Chain Analysis with Product Reviews\n",
    "\n",
    "We'll build a knowledge graph that connects:\n",
    "- **Domain Graph**: Products, Assemblies, Parts, and Suppliers (from CSV files)\n",
    "- **Subject Graph**: Extracted entities from product reviews (Issues, Features, Locations)\n",
    "- **Lexical Graph**: Document chunks with embeddings for semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Environment Setup\n",
    "\n",
    "Setting up all necessary imports and connections for both ADK and LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from itertools import islice\n",
    "\n",
    "# Google ADK imports\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "\n",
    "# Neo4j imports\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
    "from neo4j_graphrag.experimental.components.text_splitters.base import TextSplitter\n",
    "from neo4j_graphrag.experimental.components.types import TextChunk, TextChunks, PdfDocument, DocumentInfo\n",
    "from neo4j_graphrag.experimental.components.pdf_loader import DataLoader\n",
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "from neo4j_graphrag.embeddings import OpenAIEmbeddings as Neo4jOpenAIEmbeddings\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LangGraph imports for advanced workflows\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "\n",
    "# Custom workshop modules\n",
    "from neo4j_for_adk import graphdb, tool_success, tool_error\n",
    "from helper import get_neo4j_import_dir\n",
    "from tools import drop_neo4j_indexes, clear_neo4j_data\n",
    "\n",
    "# Disable warnings for clean output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get configuration from environment\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "NEO4J_IMPORT_DIR = get_neo4j_import_dir()\n",
    "\n",
    "# Verify connections\n",
    "print(f\"Neo4j URI: {NEO4J_URI}\")\n",
    "print(f\"Neo4j Username: {NEO4J_USERNAME}\")\n",
    "print(f\"Neo4j Import Dir: {NEO4J_IMPORT_DIR}\")\n",
    "print(f\"OpenAI API Key: {'✅ Set' if OPENAI_API_KEY else '❌ Missing'}\")\n",
    "\n",
    "# Test Neo4j connection\n",
    "neo4j_test = graphdb.send_query(\"RETURN 'Neo4j is Ready!' as message\")\n",
    "print(f\"\\nNeo4j Connection: {neo4j_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM clients\n",
    "MODEL_GPT_4O = \"openai/gpt-4o\"\n",
    "\n",
    "# ADK LLM\n",
    "llm_adk = LiteLlm(model=MODEL_GPT_4O)\n",
    "\n",
    "# LangChain LLM\n",
    "llm_langchain = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Neo4j GraphRAG LLM\n",
    "llm_neo4j = OpenAILLM(model_name=\"gpt-4o\", model_params={\"temperature\": 0})\n",
    "\n",
    "# Embeddings\n",
    "embedder_langchain = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "embedder_neo4j = Neo4jOpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "print(\"✅ LLM clients initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: ADK-Based Knowledge Graph Construction\n",
    "\n",
    "We'll follow the exact workflow from the notebooks but programmatically:\n",
    "1. Define user intent (supply chain analysis)\n",
    "2. Select files for import\n",
    "3. Design schema\n",
    "4. Construct domain graph from CSVs\n",
    "5. Construct subject graph from markdown reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear existing graph data (optional - be careful!)\n",
    "print(\"Clearing existing graph data...\")\n",
    "drop_neo4j_indexes()\n",
    "clear_neo4j_data()\n",
    "print(\"✅ Graph cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define User Intent and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the approved user goal (from Notebook 2)\n",
    "approved_user_goal = {\n",
    "    \"kind_of_graph\": \"supply chain analysis\",\n",
    "    \"description\": \"\"\"A multi-level bill of materials for manufactured products, \n",
    "    useful for root cause analysis. Includes product reviews to trace quality \n",
    "    issues back to suppliers and components.\"\"\"\n",
    "}\n",
    "\n",
    "# Define approved files for import (from Notebook 3)\n",
    "approved_csv_files = [\n",
    "    'products.csv',\n",
    "    'assemblies.csv', \n",
    "    'parts.csv',\n",
    "    'part_supplier_mapping.csv',\n",
    "    'suppliers.csv'\n",
    "]\n",
    "\n",
    "approved_markdown_files = [\n",
    "    \"product_reviews/gothenburg_table_reviews.md\",\n",
    "    \"product_reviews/helsingborg_dresser_reviews.md\",\n",
    "    \"product_reviews/jonkoping_coffee_table_reviews.md\",\n",
    "    \"product_reviews/linkoping_bed_reviews.md\",\n",
    "    \"product_reviews/malmo_desk_reviews.md\",\n",
    "    \"product_reviews/norrkoping_nightstand_reviews.md\",\n",
    "    \"product_reviews/orebro_lamp_reviews.md\",\n",
    "    \"product_reviews/stockholm_chair_reviews.md\",\n",
    "    \"product_reviews/uppsala_sofa_reviews.md\",\n",
    "    \"product_reviews/vasteras_bookshelf_reviews.md\"\n",
    "]\n",
    "\n",
    "print(f\"User Goal: {approved_user_goal['kind_of_graph']}\")\n",
    "print(f\"CSV Files: {len(approved_csv_files)}\")\n",
    "print(f\"Markdown Files: {len(approved_markdown_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define Schema Construction Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the approved construction plan (from Notebook 4)\n",
    "approved_construction_plan = {\n",
    "    \"Assembly\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"label\": \"Assembly\", \n",
    "        \"unique_column_name\": \"assembly_id\", \n",
    "        \"properties\": [\"assembly_name\", \"quantity\", \"product_id\"]\n",
    "    }, \n",
    "    \"Part\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"label\": \"Part\", \n",
    "        \"unique_column_name\": \"part_id\", \n",
    "        \"properties\": [\"part_name\", \"quantity\", \"assembly_id\"]\n",
    "    }, \n",
    "    \"Product\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"products.csv\", \n",
    "        \"label\": \"Product\", \n",
    "        \"unique_column_name\": \"product_id\", \n",
    "        \"properties\": [\"product_name\", \"price\", \"description\"]\n",
    "    }, \n",
    "    \"Supplier\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"suppliers.csv\", \n",
    "        \"label\": \"Supplier\", \n",
    "        \"unique_column_name\": \"supplier_id\", \n",
    "        \"properties\": [\"name\", \"specialty\", \"city\", \"country\", \"website\", \"contact_email\"]\n",
    "    }, \n",
    "    \"Contains\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"relationship_type\": \"Contains\", \n",
    "        \"from_node_label\": \"Product\", \n",
    "        \"from_node_column\": \"product_id\", \n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\", \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Is_Part_Of\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"relationship_type\": \"Is_Part_Of\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\", \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Supplied_By\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"part_supplier_mapping.csv\", \n",
    "        \"relationship_type\": \"Supplied_By\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Supplier\", \n",
    "        \"to_node_column\": \"supplier_id\", \n",
    "        \"properties\": [\"supplier_name\", \"lead_time_days\", \"unit_cost\", \"minimum_order_quantity\", \"preferred_supplier\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Construction Plan:\")\n",
    "nodes = [k for k, v in approved_construction_plan.items() if v['construction_type'] == 'node']\n",
    "relationships = [k for k, v in approved_construction_plan.items() if v['construction_type'] == 'relationship']\n",
    "print(f\"  Nodes: {nodes}\")\n",
    "print(f\"  Relationships: {relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Construct Domain Graph from CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uniqueness_constraint(label: str, unique_property_key: str) -> Dict[str, Any]:\n",
    "    \"\"\"Creates a uniqueness constraint for a node label and property key.\"\"\"\n",
    "    constraint_name = f\"{label}_{unique_property_key}_constraint\"\n",
    "    query = f\"\"\"CREATE CONSTRAINT `{constraint_name}` IF NOT EXISTS\n",
    "    FOR (n:`{label}`)\n",
    "    REQUIRE n.`{unique_property_key}` IS UNIQUE\"\"\"\n",
    "    return graphdb.send_query(query)\n",
    "\n",
    "def load_nodes_from_csv(source_file: str, label: str, unique_column_name: str, properties: list[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Load nodes from CSV file.\"\"\"\n",
    "    query = f\"\"\"LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
    "    CALL (row) {{\n",
    "        MERGE (n:$($label) {{ {unique_column_name} : row[$unique_column_name] }})\n",
    "        FOREACH (k IN $properties | SET n[k] = row[k])\n",
    "    }} IN TRANSACTIONS OF 1000 ROWS\n",
    "    \"\"\"\n",
    "    return graphdb.send_query(query, {\n",
    "        \"source_file\": source_file,\n",
    "        \"label\": label,\n",
    "        \"unique_column_name\": unique_column_name,\n",
    "        \"properties\": properties\n",
    "    })\n",
    "\n",
    "def import_nodes(node_construction: dict) -> dict:\n",
    "    \"\"\"Import nodes as defined by a node construction rule.\"\"\"\n",
    "    # Create uniqueness constraint\n",
    "    uniqueness_result = create_uniqueness_constraint(\n",
    "        node_construction[\"label\"],\n",
    "        node_construction[\"unique_column_name\"]\n",
    "    )\n",
    "    if uniqueness_result[\"status\"] == \"error\":\n",
    "        return uniqueness_result\n",
    "    \n",
    "    # Import nodes from csv\n",
    "    return load_nodes_from_csv(\n",
    "        node_construction[\"source_file\"],\n",
    "        node_construction[\"label\"],\n",
    "        node_construction[\"unique_column_name\"],\n",
    "        node_construction[\"properties\"]\n",
    "    )\n",
    "\n",
    "def import_relationships(relationship_construction: dict) -> Dict[str, Any]:\n",
    "    \"\"\"Import relationships as defined by a relationship construction rule.\"\"\"\n",
    "    from_node_column = relationship_construction[\"from_node_column\"]\n",
    "    to_node_column = relationship_construction[\"to_node_column\"]\n",
    "    \n",
    "    query = f\"\"\"LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
    "    CALL (row) {{\n",
    "        MATCH (from_node:$($from_node_label) {{ {from_node_column} : row[$from_node_column] }}),\n",
    "              (to_node:$($to_node_label) {{ {to_node_column} : row[$to_node_column] }} )\n",
    "        MERGE (from_node)-[r:$($relationship_type)]->(to_node)\n",
    "        FOREACH (k IN $properties | SET r[k] = row[k])\n",
    "    }} IN TRANSACTIONS OF 1000 ROWS\n",
    "    \"\"\"\n",
    "    \n",
    "    return graphdb.send_query(query, {\n",
    "        \"source_file\": relationship_construction[\"source_file\"],\n",
    "        \"from_node_label\": relationship_construction[\"from_node_label\"],\n",
    "        \"from_node_column\": relationship_construction[\"from_node_column\"],\n",
    "        \"to_node_label\": relationship_construction[\"to_node_label\"],\n",
    "        \"to_node_column\": relationship_construction[\"to_node_column\"],\n",
    "        \"relationship_type\": relationship_construction[\"relationship_type\"],\n",
    "        \"properties\": relationship_construction[\"properties\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_domain_graph(construction_plan: dict) -> None:\n",
    "    \"\"\"Construct the complete domain graph from CSVs.\"\"\"\n",
    "    print(\"Constructing Domain Graph...\")\n",
    "    \n",
    "    # First, import nodes\n",
    "    node_constructions = [value for value in construction_plan.values() if value['construction_type'] == 'node']\n",
    "    for node_construction in node_constructions:\n",
    "        print(f\"  Importing {node_construction['label']} nodes...\")\n",
    "        result = import_nodes(node_construction)\n",
    "        if result['status'] == 'error':\n",
    "            print(f\"    ❌ Error: {result.get('error_message', 'Unknown error')}\")\n",
    "        else:\n",
    "            print(f\"    ✅ Success\")\n",
    "    \n",
    "    # Second, import relationships\n",
    "    relationship_constructions = [value for value in construction_plan.values() if value['construction_type'] == 'relationship']\n",
    "    for relationship_construction in relationship_constructions:\n",
    "        print(f\"  Creating {relationship_construction['relationship_type']} relationships...\")\n",
    "        result = import_relationships(relationship_construction)\n",
    "        if result['status'] == 'error':\n",
    "            print(f\"    ❌ Error: {result.get('error_message', 'Unknown error')}\")\n",
    "        else:\n",
    "            print(f\"    ✅ Success\")\n",
    "\n",
    "# Construct the domain graph\n",
    "construct_domain_graph(approved_construction_plan)\n",
    "\n",
    "# Verify the domain graph\n",
    "stats = graphdb.send_query(\"\"\"\n",
    "MATCH (n)\n",
    "WITH labels(n) as labels, count(n) as count\n",
    "RETURN labels[0] as label, count\n",
    "ORDER BY label\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDomain Graph Statistics:\")\n",
    "for row in stats['query_result']:\n",
    "    print(f\"  {row['label']}: {row['count']} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define Entities and Facts for Extraction from Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define approved entities (from Notebook 5)\n",
    "approved_entities = ['Product', 'Issue', 'Feature', 'Location']\n",
    "\n",
    "# Define approved fact types (from Notebook 5)\n",
    "approved_fact_types = {\n",
    "    'has_issue': {\n",
    "        'subject_label': 'Product',\n",
    "        'predicate_label': 'has_issue',\n",
    "        'object_label': 'Issue'\n",
    "    },\n",
    "    'includes_feature': {\n",
    "        'subject_label': 'Product',\n",
    "        'predicate_label': 'includes_feature',\n",
    "        'object_label': 'Feature'\n",
    "    },\n",
    "    'used_in_location': {\n",
    "        'subject_label': 'Product',\n",
    "        'predicate_label': 'used_in_location',\n",
    "        'object_label': 'Location'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Entity Types:\", approved_entities)\n",
    "print(\"\\nFact Types:\")\n",
    "for key, fact in approved_fact_types.items():\n",
    "    print(f\"  {fact['subject_label']} --{fact['predicate_label'].upper()}--> {fact['object_label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Construct Subject Graph from Markdown Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom text splitter for markdown\n",
    "class RegexTextSplitter(TextSplitter):\n",
    "    \"\"\"Split text using regex matched delimiters.\"\"\"\n",
    "    def __init__(self, re_pattern: str):\n",
    "        self.re_pattern = re_pattern\n",
    "    \n",
    "    async def run(self, text: str) -> TextChunks:\n",
    "        texts = re.split(self.re_pattern, text)\n",
    "        chunks = [TextChunk(text=str(text), index=i) for (i, text) in enumerate(texts)]\n",
    "        return TextChunks(chunks=chunks)\n",
    "\n",
    "# Custom markdown loader\n",
    "class MarkdownDataLoader(DataLoader):\n",
    "    def extract_title(self, markdown_text):\n",
    "        pattern = r'^# (.+)$'\n",
    "        match = re.search(pattern, markdown_text, re.MULTILINE)\n",
    "        return match.group(1) if match else \"Untitled\"\n",
    "    \n",
    "    async def run(self, filepath: Path, metadata = {}) -> PdfDocument:\n",
    "        with open(filepath, \"r\") as f:\n",
    "            markdown_text = f.read()\n",
    "        doc_headline = self.extract_title(markdown_text)\n",
    "        markdown_info = DocumentInfo(\n",
    "            path=str(filepath),\n",
    "            metadata={\"title\": doc_headline}\n",
    "        )\n",
    "        return PdfDocument(text=markdown_text, document_info=markdown_info)\n",
    "\n",
    "print(\"✅ Custom loaders defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_context(file_path: str, num_lines: int = 5) -> str:\n",
    "    \"\"\"Extract first few lines of a file for context.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = []\n",
    "        for _ in range(num_lines):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append(line)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def contextualize_er_extraction_prompt(context: str) -> str:\n",
    "    \"\"\"Create entity extraction prompt with context.\"\"\"\n",
    "    general_instructions = \"\"\"\n",
    "    You are a top-tier algorithm designed for extracting\n",
    "    information in structured formats to build a knowledge graph.\n",
    "\n",
    "    Extract the entities (nodes) and specify their type from the following text.\n",
    "    Also extract the relationships between these nodes.\n",
    "\n",
    "    Return result as JSON using the following format:\n",
    "    {{\"nodes\": [ {{\"id\": \"0\", \"label\": \"Person\", \"properties\": {{\"name\": \"John\"}} }}],\n",
    "    \"relationships\": [{{\"type\": \"KNOWS\", \"start_node_id\": \"0\", \"end_node_id\": \"1\", \"properties\": {{\"since\": \"2024-08-01\"}} }}] }}\n",
    "\n",
    "    Use only the following node and relationship types (if provided):\n",
    "    {schema}\n",
    "\n",
    "    Assign a unique ID (string) to each node, and reuse it to define relationships.\n",
    "    Do respect the source and target node types for relationship and\n",
    "    the relationship direction.\n",
    "\n",
    "    Make sure you adhere to the following rules to produce valid JSON objects:\n",
    "    - Do not return any additional information other than the JSON in it.\n",
    "    - Omit any backticks around the JSON - simply output the JSON on its own.\n",
    "    - The JSON object must not wrapped into a list - it is its own JSON object.\n",
    "    - Property names must be enclosed in double quotes\n",
    "    \"\"\"\n",
    "    \n",
    "    context_section = f\"\"\"\n",
    "    Consider the following context to help identify entities and relationships:\n",
    "    <context>\n",
    "    {context}  \n",
    "    </context>\"\"\"\n",
    "    \n",
    "    input_section = \"\"\"\n",
    "    Input text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    return general_instructions + \"\\n\" + context_section + \"\\n\" + input_section\n",
    "\n",
    "print(\"✅ Extraction prompts defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_kg_builder(file_path: str) -> SimpleKGPipeline:\n",
    "    \"\"\"Build a KG builder pipeline for a given markdown file.\"\"\"\n",
    "    # Get entity schema from approved entities and facts\n",
    "    schema_node_types = approved_entities\n",
    "    schema_relationship_types = [key.upper() for key in approved_fact_types.keys()]\n",
    "    schema_patterns = [\n",
    "        [fact['subject_label'], fact['predicate_label'].upper(), fact['object_label']]\n",
    "        for fact in approved_fact_types.values()\n",
    "    ]\n",
    "    \n",
    "    entity_schema = {\n",
    "        \"node_types\": schema_node_types,\n",
    "        \"relationship_types\": schema_relationship_types,\n",
    "        \"patterns\": schema_patterns,\n",
    "        \"additional_node_types\": False\n",
    "    }\n",
    "    \n",
    "    # Create contextualized prompt\n",
    "    context = file_context(file_path)\n",
    "    contextualized_prompt = contextualize_er_extraction_prompt(context)\n",
    "    \n",
    "    # Build pipeline\n",
    "    return SimpleKGPipeline(\n",
    "        llm=llm_neo4j,\n",
    "        driver=graphdb.get_driver(),\n",
    "        embedder=embedder_neo4j,\n",
    "        from_pdf=True,\n",
    "        pdf_loader=MarkdownDataLoader(),\n",
    "        text_splitter=RegexTextSplitter(\"---\"),\n",
    "        schema=entity_schema,\n",
    "        prompt_template=contextualized_prompt,\n",
    "    )\n",
    "\n",
    "print(\"✅ KG builder function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all markdown files to create subject graph\n",
    "print(\"Constructing Subject Graph from Reviews...\")\n",
    "\n",
    "for file_name in approved_markdown_files[:3]:  # Process first 3 files for demo\n",
    "    file_path = os.path.join(NEO4J_IMPORT_DIR, file_name)\n",
    "    print(f\"  Processing: {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        kg_builder = make_kg_builder(file_path)\n",
    "        results = await kg_builder.run_async(file_path=str(file_path))\n",
    "        print(f\"    ✅ Processed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ❌ Error: {str(e)}\")\n",
    "\n",
    "print(\"\\n✅ Subject graph construction complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Entity Resolution: Connect Subject and Domain Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_subject_and_domain_nodes(label: str, entity_key: str, domain_key: str, similarity: float = 0.9) -> dict:\n",
    "    \"\"\"Correlate entity and domain nodes based on similarity.\"\"\"\n",
    "    results = graphdb.send_query(\"\"\"\n",
    "    MATCH (entity:$($entityLabel):`__Entity__`),(domain:$($entityLabel))\n",
    "    WHERE apoc.text.jaroWinklerDistance(entity[$entityKey], domain[$domainKey]) < $distance\n",
    "    MERGE (entity)-[r:CORRESPONDS_TO]->(domain)\n",
    "    ON CREATE SET r.created_at = datetime()\n",
    "    ON MATCH SET r.updated_at = datetime()\n",
    "    RETURN $entityLabel as entityLabel, count(r) as relationshipCount\n",
    "    \"\"\", {\n",
    "        \"entityLabel\": label,\n",
    "        \"entityKey\": entity_key,\n",
    "        \"domainKey\": domain_key,\n",
    "        \"distance\": (1.0 - similarity)\n",
    "    })\n",
    "    return results\n",
    "\n",
    "# Connect Product entities to Product domain nodes\n",
    "print(\"Performing Entity Resolution...\")\n",
    "result = correlate_subject_and_domain_nodes(\"Product\", \"name\", \"product_name\", similarity=0.8)\n",
    "if result['status'] == 'success':\n",
    "    count = result['query_result'][0]['relationshipCount'] if result['query_result'] else 0\n",
    "    print(f\"  ✅ Connected {count} Product entities to domain nodes\")\n",
    "else:\n",
    "    print(f\"  ❌ Error: {result.get('error_message', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LangChain GraphRAG Implementation\n",
    "\n",
    "Now we'll implement multiple retrieval strategies using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initialize LangChain Neo4j Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Neo4j Graph for LangChain\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=\"neo4j\"\n",
    ")\n",
    "\n",
    "# Refresh schema\n",
    "graph.refresh_schema()\n",
    "\n",
    "print(\"Graph Schema:\")\n",
    "print(f\"  Node Labels: {graph.structured_schema['node_props'].keys()}\")\n",
    "print(f\"  Relationships: {graph.structured_schema['relationships']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Hybrid Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector index on chunks\n",
    "create_vector_index = graphdb.send_query(\"\"\"\n",
    "CREATE VECTOR INDEX `chunk_embedding_index` IF NOT EXISTS\n",
    "FOR (c:Chunk)\n",
    "ON (c.embedding)\n",
    "OPTIONS {\n",
    "    indexConfig: {\n",
    "        `vector.dimensions`: 3072,\n",
    "        `vector.similarity_function`: 'cosine'\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Vector index creation:\", create_vector_index['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full-text index for hybrid search\n",
    "create_fulltext_index = graphdb.send_query(\"\"\"\n",
    "CREATE FULLTEXT INDEX `chunk_text_index` IF NOT EXISTS\n",
    "FOR (c:Chunk)\n",
    "ON EACH [c.text]\n",
    "\"\"\")\n",
    "\n",
    "print(\"Full-text index creation:\", create_fulltext_index['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom retrieval query that traverses the graph for context\n",
    "retrieval_query = \"\"\"\n",
    "// node is the chunk found by vector/hybrid search\n",
    "// score is the similarity score\n",
    "\n",
    "// Get parent document for broader context\n",
    "OPTIONAL MATCH (doc:Document)-[:HAS_CHUNK]->(node)\n",
    "\n",
    "// Get entities mentioned in this chunk\n",
    "OPTIONAL MATCH (entity:`__Entity__`)-[:MENTIONED_IN]->(node)\n",
    "\n",
    "// Get corresponding domain nodes\n",
    "OPTIONAL MATCH (entity)-[:CORRESPONDS_TO]->(product:Product)\n",
    "\n",
    "// Get supply chain info for products\n",
    "OPTIONAL MATCH (product)-[:Contains]->(assembly:Assembly)\n",
    "OPTIONAL MATCH (assembly)<-[:Is_Part_Of]-(part:Part)\n",
    "OPTIONAL MATCH (part)-[:Supplied_By]->(supplier:Supplier)\n",
    "\n",
    "WITH node, score, doc, \n",
    "     collect(DISTINCT entity.name) AS entities,\n",
    "     collect(DISTINCT product.product_name) AS products,\n",
    "     collect(DISTINCT supplier.name)[..3] AS suppliers\n",
    "\n",
    "RETURN \n",
    "    node.text AS text,\n",
    "    score,\n",
    "    doc.path AS source_document,\n",
    "    entities,\n",
    "    products,\n",
    "    suppliers\n",
    "\"\"\"\n",
    "\n",
    "print(\"✅ Custom retrieval query defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hybrid Vector Store with custom retrieval\n",
    "vector_store = Neo4jVector.from_existing_index(\n",
    "    embedding=embedder_langchain,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=\"chunk_embedding_index\",\n",
    "    node_label=\"Chunk\",\n",
    "    text_node_property=\"text\",\n",
    "    embedding_node_property=\"embedding\",\n",
    "    search_type=\"hybrid\",\n",
    "    retrieval_query=retrieval_query\n",
    ")\n",
    "\n",
    "print(\"✅ Hybrid vector store initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Implement Multiple Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupplyChainRAG:\n",
    "    \"\"\"Multi-strategy retrieval for supply chain Q&A.\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, graph, llm):\n",
    "        self.vector_store = vector_store\n",
    "        self.graph = graph\n",
    "        self.llm = llm\n",
    "        self.cypher_chain = self._create_cypher_chain()\n",
    "    \n",
    "    def _create_cypher_chain(self):\n",
    "        \"\"\"Create GraphCypherQAChain with custom prompt.\"\"\"\n",
    "        cypher_prompt_template = \"\"\"\n",
    "        You are an expert at converting questions about supply chain and product quality\n",
    "        into Neo4j Cypher queries.\n",
    "        \n",
    "        The graph contains:\n",
    "        - Product nodes with properties: product_id, product_name, price, description\n",
    "        - Assembly nodes with properties: assembly_id, assembly_name, quantity\n",
    "        - Part nodes with properties: part_id, part_name, quantity\n",
    "        - Supplier nodes with properties: supplier_id, name, city, country, specialty\n",
    "        - Relationships: (Product)-[:Contains]->(Assembly), (Part)-[:Is_Part_Of]->(Assembly), (Part)-[:Supplied_By]->(Supplier)\n",
    "        - Entity nodes from reviews with labels: Issue, Feature, Location\n",
    "        \n",
    "        Schema: {schema}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Return a valid Cypher query that answers the question.\n",
    "        \"\"\"\n",
    "        \n",
    "        cypher_prompt = PromptTemplate(\n",
    "            template=cypher_prompt_template,\n",
    "            input_variables=[\"schema\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        return GraphCypherQAChain.from_llm(\n",
    "            llm=self.llm,\n",
    "            graph=self.graph,\n",
    "            cypher_prompt=cypher_prompt,\n",
    "            verbose=True,\n",
    "            validate_cypher=True,\n",
    "            top_k=10\n",
    "        )\n",
    "    \n",
    "    def hybrid_search(self, question: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Strategy 1: Hybrid vector + full-text search with graph traversal.\"\"\"\n",
    "        return self.vector_store.similarity_search(question, k=k)\n",
    "    \n",
    "    def cypher_query(self, question: str) -> str:\n",
    "        \"\"\"Strategy 2: Direct Cypher query generation.\"\"\"\n",
    "        try:\n",
    "            result = self.cypher_chain.invoke({\"query\": question})\n",
    "            return result[\"result\"]\n",
    "        except Exception as e:\n",
    "            return f\"Error executing Cypher query: {str(e)}\"\n",
    "    \n",
    "    def trace_issue_to_supplier(self, product_name: str, issue: str) -> str:\n",
    "        \"\"\"Strategy 3: Specialized query for root cause analysis.\"\"\"\n",
    "        cypher = \"\"\"\n",
    "        MATCH (p:Product {product_name: $product_name})\n",
    "        MATCH (p)-[:Contains]->(a:Assembly)\n",
    "        MATCH (part:Part)-[:Is_Part_Of]->(a)\n",
    "        MATCH (part)-[:Supplied_By]->(s:Supplier)\n",
    "        \n",
    "        OPTIONAL MATCH (issue:Issue)\n",
    "        WHERE issue.name CONTAINS $issue\n",
    "        \n",
    "        RETURN DISTINCT \n",
    "            p.product_name AS product,\n",
    "            a.assembly_name AS assembly,\n",
    "            collect(DISTINCT part.part_name)[..5] AS parts,\n",
    "            collect(DISTINCT s.name)[..5] AS suppliers,\n",
    "            collect(DISTINCT s.city + ', ' + s.country)[..5] AS locations\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.graph.query(cypher, {\n",
    "            \"product_name\": product_name,\n",
    "            \"issue\": issue\n",
    "        })\n",
    "        \n",
    "        return self._format_trace_results(result)\n",
    "    \n",
    "    def _format_trace_results(self, results):\n",
    "        \"\"\"Format tracing results into readable text.\"\"\"\n",
    "        if not results:\n",
    "            return \"No supply chain information found.\"\n",
    "        \n",
    "        output = []\n",
    "        for r in results:\n",
    "            output.append(f\"Product: {r['product']}\")\n",
    "            output.append(f\"Assembly: {r['assembly']}\")\n",
    "            output.append(f\"Parts: {', '.join(r['parts'])}\")\n",
    "            output.append(f\"Suppliers: {', '.join(r['suppliers'])}\")\n",
    "            output.append(f\"Locations: {', '.join(r['locations'])}\")\n",
    "            output.append(\"---\")\n",
    "        \n",
    "        return \"\\n\".join(output)\n",
    "    \n",
    "    def answer_question(self, question: str) -> str:\n",
    "        \"\"\"Combined strategy: Use multiple retrieval methods and synthesize answer.\"\"\"\n",
    "        # Get context from hybrid search\n",
    "        docs = self.hybrid_search(question, k=3)\n",
    "        vector_context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Try Cypher query for structured data\n",
    "        cypher_result = self.cypher_query(question)\n",
    "        \n",
    "        # Generate final answer\n",
    "        prompt = f\"\"\"\n",
    "        Answer the following question using the provided context.\n",
    "        \n",
    "        Context from reviews and documentation:\n",
    "        {vector_context}\n",
    "        \n",
    "        Context from structured data:\n",
    "        {cypher_result}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Provide a comprehensive answer that combines insights from both sources.\n",
    "        If there are quality issues mentioned, trace them to potential root causes.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(prompt)\n",
    "        return response.content\n",
    "\n",
    "# Initialize the RAG system\n",
    "rag_system = SupplyChainRAG(vector_store, graph, llm_langchain)\n",
    "print(\"✅ Supply Chain RAG system initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Advanced Features with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state for the workflow\n",
    "class RAGState(TypedDict):\n",
    "    question: str\n",
    "    query_type: str\n",
    "    vector_results: List[str]\n",
    "    cypher_results: str\n",
    "    final_context: str\n",
    "    answer: str\n",
    "\n",
    "# Workflow functions\n",
    "def classify_query(state: RAGState) -> RAGState:\n",
    "    \"\"\"Classify the query type.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Simple classification logic\n",
    "    if any(word in question.lower() for word in [\"supplier\", \"part\", \"assembly\"]):\n",
    "        state[\"query_type\"] = \"structured\"\n",
    "    elif any(word in question.lower() for word in [\"issue\", \"problem\", \"quality\", \"review\"]):\n",
    "        state[\"query_type\"] = \"unstructured\"\n",
    "    else:\n",
    "        state[\"query_type\"] = \"both\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "def vector_search(state: RAGState) -> RAGState:\n",
    "    \"\"\"Perform vector search.\"\"\"\n",
    "    docs = rag_system.hybrid_search(state[\"question\"], k=3)\n",
    "    state[\"vector_results\"] = [doc.page_content for doc in docs]\n",
    "    return state\n",
    "\n",
    "def cypher_search(state: RAGState) -> RAGState:\n",
    "    \"\"\"Execute Cypher search.\"\"\"\n",
    "    state[\"cypher_results\"] = rag_system.cypher_query(state[\"question\"])\n",
    "    return state\n",
    "\n",
    "def aggregate_results(state: RAGState) -> RAGState:\n",
    "    \"\"\"Combine results from multiple sources.\"\"\"\n",
    "    context_parts = []\n",
    "    \n",
    "    if state.get(\"vector_results\"):\n",
    "        context_parts.append(\"From reviews and documentation:\")\n",
    "        context_parts.extend(state[\"vector_results\"])\n",
    "    \n",
    "    if state.get(\"cypher_results\"):\n",
    "        context_parts.append(\"\\nFrom structured data:\")\n",
    "        context_parts.append(state[\"cypher_results\"])\n",
    "    \n",
    "    state[\"final_context\"] = \"\\n\\n\".join(context_parts)\n",
    "    return state\n",
    "\n",
    "def generate_answer(state: RAGState) -> RAGState:\n",
    "    \"\"\"Generate final answer.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Answer this question based on the context:\n",
    "    \n",
    "    Context:\n",
    "    {state['final_context']}\n",
    "    \n",
    "    Question: {state['question']}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm_langchain.invoke(prompt)\n",
    "    state[\"answer\"] = response.content\n",
    "    return state\n",
    "\n",
    "# Build the workflow\n",
    "workflow = StateGraph(RAGState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"classify\", classify_query)\n",
    "workflow.add_node(\"vector\", vector_search)\n",
    "workflow.add_node(\"cypher\", cypher_search)\n",
    "workflow.add_node(\"aggregate\", aggregate_results)\n",
    "workflow.add_node(\"generate\", generate_answer)\n",
    "\n",
    "# Add edges with conditional routing\n",
    "workflow.set_entry_point(\"classify\")\n",
    "\n",
    "def route_after_classification(state):\n",
    "    if state[\"query_type\"] == \"structured\":\n",
    "        return \"cypher\"\n",
    "    elif state[\"query_type\"] == \"unstructured\":\n",
    "        return \"vector\"\n",
    "    else:\n",
    "        return \"vector\"  # Start with vector for \"both\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify\",\n",
    "    route_after_classification,\n",
    "    {\n",
    "        \"vector\": \"vector\",\n",
    "        \"cypher\": \"cypher\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Connect the rest of the workflow\n",
    "workflow.add_edge(\"vector\", \"cypher\")  # After vector, also do cypher\n",
    "workflow.add_edge(\"cypher\", \"aggregate\")\n",
    "workflow.add_edge(\"aggregate\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"✅ LangGraph workflow compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Interactive Demo\n",
    "\n",
    "Test the system with various queries to demonstrate different retrieval strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries demonstrating different capabilities\n",
    "test_queries = [\n",
    "    \"Which suppliers are responsible for quality issues in the Uppsala Sofa?\",\n",
    "    \"What are the most common problems across all furniture products?\",\n",
    "    \"Find all products that use parts from suppliers in Sweden\",\n",
    "    \"Which assembly has the most customer complaints?\",\n",
    "    \"What quality issues are mentioned in the Stockholm Chair reviews?\",\n",
    "    \"List all suppliers and their specialties\",\n",
    "    \"How many parts does the Gothenburg Table have?\"\n",
    "]\n",
    "\n",
    "print(\"Test Queries:\")\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_retrieval_strategy(question: str, strategy: str = \"all\"):\n",
    "    \"\"\"Test different retrieval strategies.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Strategy: {strategy}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if strategy == \"hybrid\" or strategy == \"all\":\n",
    "        print(\"\\n--- Hybrid Search Results ---\")\n",
    "        docs = rag_system.hybrid_search(question, k=2)\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            print(f\"\\nResult {i}:\")\n",
    "            print(f\"  Text: {doc.page_content[:200]}...\")\n",
    "            if doc.metadata:\n",
    "                print(f\"  Metadata: {doc.metadata}\")\n",
    "    \n",
    "    if strategy == \"cypher\" or strategy == \"all\":\n",
    "        print(\"\\n--- Cypher Query Results ---\")\n",
    "        result = rag_system.cypher_query(question)\n",
    "        print(result)\n",
    "    \n",
    "    if strategy == \"combined\" or strategy == \"all\":\n",
    "        print(\"\\n--- Combined Answer ---\")\n",
    "        answer = rag_system.answer_question(question)\n",
    "        print(answer)\n",
    "    \n",
    "    if strategy == \"workflow\":\n",
    "        print(\"\\n--- LangGraph Workflow ---\")\n",
    "        result = app.invoke({\"question\": question})\n",
    "        print(f\"Query Type: {result['query_type']}\")\n",
    "        print(f\"\\nAnswer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test first query with all strategies\n",
    "test_retrieval_strategy(test_queries[0], \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test workflow on a complex query\n",
    "test_retrieval_strategy(test_queries[1], \"workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_qa():\n",
    "    \"\"\"Interactive Q&A interface.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Supply Chain Knowledge Graph Q&A System\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nType 'quit' to exit, 'help' for example queries\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nYour question: \").strip()\n",
    "        \n",
    "        if question.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if question.lower() == 'help':\n",
    "            print(\"\\nExample queries:\")\n",
    "            for q in test_queries[:3]:\n",
    "                print(f\"  - {q}\")\n",
    "            continue\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Use the workflow for comprehensive answers\n",
    "            print(\"\\nProcessing...\")\n",
    "            result = app.invoke({\"question\": question})\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*40)\n",
    "            print(\"Answer:\")\n",
    "            print(\"-\"*40)\n",
    "            print(result['answer'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")\n",
    "            print(\"Falling back to simple search...\")\n",
    "            answer = rag_system.answer_question(question)\n",
    "            print(answer)\n",
    "\n",
    "# Run interactive interface\n",
    "# interactive_qa()  # Uncomment to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_subgraph(product_name: str):\n",
    "    \"\"\"Generate Cypher for visualizing a product's supply chain.\"\"\"\n",
    "    cypher = f\"\"\"\n",
    "    // Visualization query for {product_name}\n",
    "    MATCH path = (p:Product {{product_name: '{product_name}'}})-[:Contains]->(a:Assembly)\n",
    "    OPTIONAL MATCH parts_path = (part:Part)-[:Is_Part_Of]->(a)\n",
    "    OPTIONAL MATCH supplier_path = (part)-[:Supplied_By]->(s:Supplier)\n",
    "    OPTIONAL MATCH entity_path = (e:`__Entity__`:Product)-[:CORRESPONDS_TO]->(p)\n",
    "    OPTIONAL MATCH issue_path = (e)-[:HAS_ISSUE]->(issue:Issue)\n",
    "    \n",
    "    RETURN path, parts_path, supplier_path, entity_path, issue_path\n",
    "    LIMIT 50\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"To visualize {product_name} supply chain in Neo4j Browser, run:\")\n",
    "    print(\"\\n\" + cypher)\n",
    "    \n",
    "    # Get statistics\n",
    "    stats_cypher = f\"\"\"\n",
    "    MATCH (p:Product {{product_name: '{product_name}'}})\n",
    "    OPTIONAL MATCH (p)-[:Contains]->(a:Assembly)\n",
    "    OPTIONAL MATCH (part:Part)-[:Is_Part_Of]->(a)\n",
    "    OPTIONAL MATCH (part)-[:Supplied_By]->(s:Supplier)\n",
    "    RETURN \n",
    "        count(DISTINCT a) as assemblies,\n",
    "        count(DISTINCT part) as parts,\n",
    "        count(DISTINCT s) as suppliers\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = graph.query(stats_cypher)\n",
    "    if stats:\n",
    "        print(f\"\\nSupply Chain Statistics for {product_name}:\")\n",
    "        print(f\"  Assemblies: {stats[0]['assemblies']}\")\n",
    "        print(f\"  Parts: {stats[0]['parts']}\")\n",
    "        print(f\"  Suppliers: {stats[0]['suppliers']}\")\n",
    "\n",
    "# Example visualization\n",
    "visualize_subgraph(\"Uppsala Sofa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Evaluation\n",
    "\n",
    "### What We've Built:\n",
    "\n",
    "1. **Complete Knowledge Graph**:\n",
    "   - Domain graph from structured CSV data\n",
    "   - Subject graph from unstructured markdown reviews\n",
    "   - Lexical graph with embeddings for semantic search\n",
    "   - Entity resolution linking graphs together\n",
    "\n",
    "2. **Multiple Retrieval Strategies**:\n",
    "   - Hybrid search (vector + full-text) with graph traversal\n",
    "   - Cypher generation for structured queries\n",
    "   - Specialized tracing for root cause analysis\n",
    "   - LangGraph workflow for intelligent routing\n",
    "\n",
    "3. **Supply Chain Q&A Capabilities**:\n",
    "   - Trace quality issues to suppliers\n",
    "   - Analyze patterns across products\n",
    "   - Combine structured and unstructured data\n",
    "   - Provide context-rich answers\n",
    "\n",
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final graph statistics\n",
    "final_stats = graphdb.send_query(\"\"\"\n",
    "MATCH (n)\n",
    "WITH labels(n) as node_labels, count(n) as count\n",
    "UNWIND node_labels as label\n",
    "WITH label, sum(count) as total\n",
    "RETURN label, total\n",
    "ORDER BY total DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Final Knowledge Graph Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "total_nodes = 0\n",
    "for row in final_stats['query_result']:\n",
    "    print(f\"{row['label']:20} {row['total']:10,} nodes\")\n",
    "    total_nodes += row['total']\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Total':20} {total_nodes:10,} nodes\")\n",
    "\n",
    "# Count relationships\n",
    "rel_stats = graphdb.send_query(\"\"\"\n",
    "MATCH ()-[r]->()\n",
    "RETURN type(r) as type, count(r) as count\n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nRelationship Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "total_rels = 0\n",
    "for row in rel_stats['query_result']:\n",
    "    print(f\"{row['type']:20} {row['count']:10,} relationships\")\n",
    "    total_rels += row['count']\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Total':20} {total_rels:10,} relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Complete pipeline** from raw data to working Q&A system\n",
    "2. **Integration** of ADK workflow with LangChain GraphRAG\n",
    "3. **Multiple retrieval strategies** for different query types\n",
    "4. **Real-world use case** with supply chain analysis\n",
    "\n",
    "The system can now:\n",
    "- Answer complex supply chain questions\n",
    "- Trace quality issues to root causes\n",
    "- Combine insights from reviews and structured data\n",
    "- Provide context-aware, accurate responses\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Process all review files (we only did 3 for demo)\n",
    "2. Add more sophisticated entity resolution\n",
    "3. Implement evaluation metrics\n",
    "4. Deploy as API service\n",
    "5. Add caching for performance\n",
    "6. Create dashboard for visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}